{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 Declare utility functions for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "def display_images_in_table(img_set, label_set, color_map=None, max_columns=3, figsize=(20, 14), show_axes=True) :\n",
    "    '''\n",
    "    Display provided array of images in a form of the table.\n",
    "    Signs each displayed image with corresponding label from provided arry of string labels\n",
    "    img_set - array of images to display\n",
    "    label_set - array of string labels to sign the images. Note!: Must be same size as array of images\n",
    "    color_map - color map to use for visualization. If not provided - RGB will be used\n",
    "    '''\n",
    "    assert(len(img_set) == len(label_set))\n",
    "    rows = (len(label_set) / max_columns) + 1\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(len(img_set)):\n",
    "        label = label_set[i]\n",
    "        # Each subplot is an image with its label    \n",
    "        a = plt.subplot(rows, max_columns, i + 1)\n",
    "        plt.title(label)\n",
    "        # get correspondent image from image set \n",
    "        if (len(img_set[i].shape) == 1):\n",
    "            image = np.ones((np.amax(img_set[i] * 6), img_set[i].shape[0]), dtype=np.int32)\n",
    "            y = list(map(lambda x:6*x, img_set[i]))\n",
    "            for j in range(y.shape[0]):\n",
    "                image[y[j], j] = 0\n",
    "        else:\n",
    "            image = img_set[i].squeeze()   \n",
    "        fig = plt.imshow(image, cmap='gray')\n",
    "        if(not show_axes):\n",
    "            # hide image size scale axes\n",
    "            fig.axes.get_xaxis().set_visible(False)\n",
    "            fig.axes.get_yaxis().set_visible(False) \n",
    "        \n",
    "    plt.show() \n",
    "    \n",
    "def display_image_and_plot(image, plot, image_label, plot_label):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    fig = plt.imshow(image)\n",
    "    plt.title(image_label)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    fig = plt.plot(plot)\n",
    "    plt.title(plot_label)\n",
    "    plt.show()\n",
    "\n",
    "def display_dataset_samples(n_images_per_class, cars, notcars):\n",
    "    sample_cars = []\n",
    "    sample_car_labels = []\n",
    "    sample_notcars = []\n",
    "    sample_notcar_labels = []\n",
    "    for i in range(n_images_per_class):\n",
    "        car_file = cars[np.random.randint(len(cars))]\n",
    "        sample_cars.append(mpimg.imread(car_file))\n",
    "        sample_car_labels.append(car_file.split(\"/\")[-1])\n",
    "        \n",
    "        notcar_file = notcars[np.random.randint(len(notcars))]\n",
    "        sample_notcars.append(mpimg.imread(notcar_file))\n",
    "        sample_notcar_labels.append(notcar_file.split(\"/\")[-1])\n",
    "    print(\"Random {} 'car' images from dataset:\".format(n_images_per_class))    \n",
    "    display_images_in_table(sample_cars, sample_car_labels, max_columns=5, figsize=(20,10), show_axes=False)   \n",
    "    print(\"Random {} 'not a car' images from dataset:\".format(n_images_per_class))    \n",
    "    display_images_in_table(sample_notcars, sample_notcar_labels, max_columns=5, figsize=(20,10), show_axes=False)\n",
    "    \n",
    "def display_spatial_binning_of_color(image_file):\n",
    "    image = mpimg.imread(image_file)\n",
    "    if(image_file.endswith(\".png\")): \n",
    "            image = (image*255).astype(np.uint8)\n",
    "    # call feature vector extraction function        \n",
    "    feature_vec = bin_spatial(image, size=(32, 32))\n",
    "    # show results\n",
    "    display_image_and_plot(image, feature_vec, \"Original Image\", \"Spatially Binned Color channels\")\n",
    "\n",
    "def display_histograms_of_colors(image_file):\n",
    "    image = mpimg.imread(image_file)\n",
    "    if(image_file.endswith(\".png\")): \n",
    "            image = (image*255).astype(np.uint8)\n",
    "    # call feature vector extraction function      \n",
    "    r_hist, g_hist, b_hist, bin_centers, feature_vec = color_hist(image, nbins=64, bins_range=(0, 256), vis=True)\n",
    "    # show results\n",
    "    fig = plt.figure(figsize=(12,3))\n",
    "    plt.subplot(141)\n",
    "    fig = plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.subplot(142)\n",
    "    plt.bar(bin_centers, r_hist[0])\n",
    "    plt.xlim(0, 256)\n",
    "    plt.title('R channel histogram')\n",
    "    plt.subplot(143)\n",
    "    plt.bar(bin_centers, g_hist[0])\n",
    "    plt.xlim(0, 256)\n",
    "    plt.title('G channel histogram')\n",
    "    plt.subplot(144)\n",
    "    plt.bar(bin_centers, b_hist[0])\n",
    "    plt.xlim(0, 256)\n",
    "    plt.title('B channel histogram')\n",
    "    plt.show()\n",
    "    \n",
    "def display_histograms_of_gradients(image_file):\n",
    "    image = mpimg.imread(image_file)\n",
    "    if(image_file.endswith(\".png\")): \n",
    "            image = (image*255).astype(np.uint8)\n",
    "    grayscale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # dummy HOG parameters\n",
    "    orient = 8\n",
    "    pix_per_cell = 8\n",
    "    cell_per_block = 5\n",
    "    # call feature vector extraction function \n",
    "    features, hog_image = get_hog_features(grayscale, orient, \n",
    "                        pix_per_cell, cell_per_block, \n",
    "                        vis=True, feature_vec=False)\n",
    "    # show results\n",
    "    display_images_in_table([image, hog_image], [\"Original image\",\"Histogram of Gradients\"],figsize=(12,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "car_images = glob.glob('./dataset/vehicles/**/*.png', recursive=True)\n",
    "cars = []\n",
    "for image_file_path in car_images:\n",
    "    cars.append(image_file_path)\n",
    "print(\"Number of images with vehicles: \", len(cars))     \n",
    "    \n",
    "notcar_images = glob.glob('./dataset/non-vehicles/**/*.png', recursive=True)\n",
    "notcars = []    \n",
    "for image_file_path in notcar_images:\n",
    "    notcars.append(image_file_path)\n",
    "print(\"Number of images without vehicles: \", len(notcars))\n",
    "\n",
    "# visualization of  random images from both 'car'and 'not car' datasets (10 images each)\n",
    "display_dataset_samples(10, cars, notcars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 Generating feature vector from images using Spatial Binning of color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    '''\n",
    "    Returns the vector of featues extracted from given image using Spatial Binnig of Color channels\n",
    "    img - image to extract features\n",
    "    size - target size for imge resizing\n",
    "    '''\n",
    "    # decrease the image resolution (separately ech color channel) by resizing to given size\n",
    "    # and flatten resulting imges to a 1D feature vector\n",
    "    color1 = cv2.resize(img[:,:,0], size).ravel()\n",
    "    color2 = cv2.resize(img[:,:,1], size).ravel()\n",
    "    color3 = cv2.resize(img[:,:,2], size).ravel()\n",
    "    # combine generated feature fectors into single vector\n",
    "    return np.hstack((color1, color2, color3))\n",
    "\n",
    "print(\"Feature vector generated using Spatial Binning of Color channles of random 'car' image\")\n",
    "display_spatial_binning_of_color(cars[np.random.randint(len(cars))])\n",
    "print(\"Feature vector generated using patial Binning of Color channles of random 'not car' image\")\n",
    "display_spatial_binning_of_color(notcars[np.random.randint(len(notcars))])\n",
    "\n",
    "# global configuration parameter for Spatial Binning of Colors based feature set extration for training data generation\n",
    "SPATIAL_SIZE = (32, 32) # Spatial binning dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 Generate feature vector from images using Histogram of Color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def color_hist(img, nbins=32, bins_range=(0, 256), vis=False):\n",
    "    '''\n",
    "    Returns the vector of featues extracted from given image using Color Histogram method\n",
    "    img - image to extract features\n",
    "    nbins - number of bins to split the pixel values of each color channel\n",
    "    '''\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    \n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    \n",
    "    if(vis == True):\n",
    "           # Generating bin centers\n",
    "        bin_edges = channel1_hist[1]\n",
    "        bin_centers = (bin_edges[1:]  + bin_edges[0:len(bin_edges)-1])/2\n",
    "        # Concatenate the histograms into a single feature vector\n",
    "        return channel1_hist, channel2_hist, channel3_hist, bin_centers, hist_features\n",
    "    else:\n",
    "        return hist_features \n",
    "\n",
    "print(\"Histograms of color channles of random 'car' image\")\n",
    "display_histograms_of_colors(cars[np.random.randint(len(cars))])\n",
    "print(\"Histograms of color channles of random 'not car' image\")\n",
    "display_histograms_of_colors(notcars[np.random.randint(len(notcars))])\n",
    "\n",
    "# global configuration parameter for Histogram of Color based feature set xstarction for training data generation\n",
    "N_HISTOGRAM_BINS = 32    # Number of histogram bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 Generate feature vector from images using Histogram of Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    '''\n",
    "    Extract the features from given images generated using Histogram Of Gradients method\n",
    "    \n",
    "    img is image to featch the feature vector\n",
    "    orient is a number of orientation bins\n",
    "    pix_per_cell is a number of pixels per cell (e.g. 32 means a suare cell of 32x32 pixels)\n",
    "    cell_per_block is number of cells per block (e.g 8 means a square block of 8x8 cells)\n",
    "    vis is a boolean flag declaring whether the visualization image of extracted features should be returned as well\n",
    "    feature_vec is a boolean flag declaring whether array of features should beflattened into the 1D vector\n",
    "    '''\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=False, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=False, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features   \n",
    "    \n",
    "print(\"Histogram of gradients of random 'car' image\")\n",
    "display_histograms_of_gradients(cars[np.random.randint(len(cars))])\n",
    "print(\"Histogram of gradients of random 'not car' image\")\n",
    "display_histograms_of_gradients(notcars[np.random.randint(len(notcars))])  \n",
    "\n",
    "# global configuration parameters for hog based feture set extraction of training data preparat\n",
    "COLOR_SPACE = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "N_ORIENTATIONS = 12 # HOG orientations\n",
    "N_PIXELS_PER_CELL = 8 # HOG pixels per cell\n",
    "N_CELLS_PER_BLOCK = 2 # HOG cells per block\n",
    "HOG_CHANNEL = \"ALL\" # Can be 0, 1, 2, or \"ALL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6 Generate a training and test feature and lebel sets from loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def change_color_space(rgb_image, color_space='RGB'):\n",
    "    '''\n",
    "    Changes the color space of given RGB image to provided color space\n",
    "    rgb_image - image to convert. should be provided in 0-255 pixel value scale\n",
    "    color_space - target convertion color space\n",
    "    '''\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            return cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            return cv2.cvtColor(rgb_image, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            return cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            return cv2.cvtColor(rgb_image, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            return cv2.cvtColor(rgb_image, cv2.COLOR_RGB2YCrCb)\n",
    "    else: return np.copy(rgb_image) \n",
    "\n",
    "\n",
    "def extract_features(image_files, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):\n",
    "    # check the dataset size\n",
    "    assert(len(image_files) > 0)\n",
    "    # Resulting list to populate with feature vectors\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for image_file in image_files:\n",
    "        image_features = []\n",
    "        # Read the emage\n",
    "        image = mpimg.imread(image_file)\n",
    "        # rescale pixel values to [0:255] if image is PNG (matplot read ong in [0:1] pixel value scale)\n",
    "        if(image_file.endswith(\".png\")): \n",
    "            image = (image*255).astype(np.uint8)\n",
    "        # apply conversion of image to target color space\n",
    "        image = change_color_space(image, color_space)      \n",
    "        \n",
    "        if spatial_feat == True:\n",
    "            # genereta feature fector using Spatial Binning of Colors method \n",
    "            spatial_features = bin_spatial(image, size=spatial_size)\n",
    "            image_features.append(spatial_features)\n",
    "        \n",
    "        if hist_feat == True:\n",
    "            # genereta feature fector using Histogram of Colors method \n",
    "            hist_features = color_hist(image, nbins=hist_bins)\n",
    "            image_features.append(hist_features)\n",
    "        \n",
    "        if hog_feat == True:\n",
    "            # generate feature fector using Histogram of Gradients method \n",
    "            if hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(image[:,:,channel], \n",
    "                                        orient, pix_per_cell, cell_per_block, \n",
    "                                        vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)\n",
    "            else:\n",
    "                hog_features = get_hog_features(image[:,:,hog_channel], orient, \n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            image_features.append(hog_features)\n",
    "        \n",
    "        features.append(np.concatenate(image_features))\n",
    "        # print the sizeof generated feature vectors (only for first image in a dataset, since all other will be teh same)\n",
    "        if(image_file == image_files[0]):\n",
    "            print(\"... spatial features vector size: \", spatial_features.shape)\n",
    "            print(\"... color histogram features vector size: \", hist_features.shape)\n",
    "            print(\"... HOG features vector size (multichannel): \", hog_features.shape)\n",
    "            \n",
    "    print(\"... total feature vector size: \", features[0].shape)\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "SPATIAL_BINNING_FEATURES_ON = True # Spatial features on or off\n",
    "COLOR_HISTOGRAM_FEATURES_ON = True # Histogram features on or off\n",
    "HOG_FEATURES_ON = True # HOG features on or off\n",
    "print(\"Feature set extraction params:\") \n",
    "if (HOG_FEATURES_ON):\n",
    "    print(\"...HOG: Number of orientation bins: \", N_ORIENTATIONS)\n",
    "    print(\"...HOG: Pixels per cell: {}x{}\".format(N_PIXELS_PER_CELL, N_PIXELS_PER_CELL))\n",
    "    print(\"...HOG: Cells per block: {}x{}\".format(N_CELLS_PER_BLOCK, N_CELLS_PER_BLOCK))\n",
    "\n",
    "if(SPATIAL_BINNING_FEATURES_ON):\n",
    "    print(\"...Spatial Binning of Colors: binning size: \",  SPATIAL_SIZE)\n",
    "\n",
    "if(COLOR_HISTOGRAM_FEATURES_ON):      \n",
    "    print(\"...Histogram of Colors: number of histogram bins \",  N_HISTOGRAM_BINS)\n",
    "\n",
    "print(\"Fetching feature set for 'car' images...\")\n",
    "car_features = extract_features(cars, color_space=COLOR_SPACE, \n",
    "                        spatial_size=SPATIAL_SIZE, hist_bins=N_HISTOGRAM_BINS, \n",
    "                        orient=N_ORIENTATIONS, pix_per_cell=N_PIXELS_PER_CELL, \n",
    "                        cell_per_block=N_CELLS_PER_BLOCK, \n",
    "                        hog_channel=HOG_CHANNEL, spatial_feat=SPATIAL_BINNING_FEATURES_ON, \n",
    "                        hist_feat=COLOR_HISTOGRAM_FEATURES_ON, hog_feat=HOG_FEATURES_ON)\n",
    "\n",
    "print(\"Fetching feature set for 'not car' images...\")\n",
    "notcar_features = extract_features(notcars, color_space=COLOR_SPACE, \n",
    "                        spatial_size=SPATIAL_SIZE, hist_bins=N_HISTOGRAM_BINS, \n",
    "                        orient=N_ORIENTATIONS, pix_per_cell=N_PIXELS_PER_CELL, \n",
    "                        cell_per_block=N_CELLS_PER_BLOCK, \n",
    "                        hog_channel=HOG_CHANNEL, spatial_feat=SPATIAL_BINNING_FEATURES_ON, \n",
    "                        hist_feat=COLOR_HISTOGRAM_FEATURES_ON, hog_feat=HOG_FEATURES_ON)\n",
    "\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)  \n",
    "\n",
    "# Normalize the combined feature vector\n",
    "X_scaler = StandardScaler().fit(X) # Fit a scaler with a feture vector\n",
    "scaled_X = X_scaler.transform(X) # Apply the scaler to collected training data\n",
    "print(\"Final size of normalized features dataset: \", scaled_X.shape)\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "print(\"Size of training dataset: \", X_train.shape)\n",
    "print(\"Size of test dataset: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7 Train the SVC classifier using generated training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import time\n",
    "\n",
    "# Use a linear Support Vector Classifier\n",
    "svc = LinearSVC()\n",
    "# Check the training time\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "print(\"Training duration: {} seconds\".format(round(time.time()-t, 2)))\n",
    "# Check the accuracy score on test dataset\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#8 Sliding windows search algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def overlay_rectangles_on_image(image, rectangles, color=(0, 0, 255), thickness=6):\n",
    "    out_image = np.copy(image)\n",
    "    for rectangle in rectangles:\n",
    "        cv2.rectangle(out_image, rectangle[0], rectangle[1], color, thickness)\n",
    "    return out_image\n",
    "\n",
    "# Define a single function that can extract features using hog sub-sampling and make predictions\n",
    "def find_car_matched_windows(img, color_space, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, spatial_size, hist_bins):\n",
    "    \n",
    "    draw_img = np.copy(img)\n",
    "    \n",
    "    search_area = img[ystart:ystop,:,:]\n",
    "    search_area_color_converted = change_color_space(search_area, color_space)\n",
    "    if scale != 1:\n",
    "        imshape = search_area_color_converted.shape\n",
    "        search_area_color_converted = cv2.resize(search_area_color_converted, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "    \n",
    "    ch1 = search_area_color_converted[:,:,0]\n",
    "    ch2 = search_area_color_converted[:,:,1]\n",
    "    ch3 = search_area_color_converted[:,:,2]\n",
    "\n",
    "    # Define blocks and sliding teps\n",
    "    nxblocks = (ch1.shape[1] // pix_per_cell) - cell_per_block + 1\n",
    "    nyblocks = (ch1.shape[0] // pix_per_cell) - cell_per_block + 1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "    \n",
    "    window_size = 64 # 8 cells and 8 pix per cell\n",
    "    cells_per_step = 2  # Instead of defiingn the overlap percentage, we define how many cells to shift one the next step\n",
    "    \n",
    "    nblocks_per_window = (window_size // pix_per_cell) - cell_per_block + 1\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step + 1\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step + 1\n",
    "    \n",
    "    # Compute individual channel HOG features for the entire search area\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    \n",
    "    bounding_boxes=[]\n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(search_area_color_converted[ytop:ytop+window_size, xleft:xleft+window_size], (64,64))\n",
    "            \n",
    "            # Get color features\n",
    "            spatial_features = bin_spatial(subimg, size=spatial_size)\n",
    "            hist_features = color_hist(subimg, nbins=hist_bins)\n",
    " \n",
    "            # Scale features and make a prediction\n",
    "            pred_X = np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1)\n",
    "            test_features = X_scaler.transform(pred_X)  \n",
    "            \n",
    "            test_prediction = svc.predict(test_features)\n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window_size*scale)\n",
    "                detection_bounding_box = [(xbox_left, ytop_draw+ystart),(xbox_left+win_draw,ytop_draw+win_draw+ystart)]\n",
    "                bounding_boxes.append(detection_bounding_box)\n",
    "            \n",
    "    return bounding_boxes\n",
    "\n",
    "\n",
    "# IMAGE FOR TESTING THE PLIPELINE\n",
    "image = mpimg.imread('./test_images/bbox-example-image.jpg')\n",
    "\n",
    "# top veritical bound for vehicle detection\n",
    "y_start = 502\n",
    "# bottom veritical bound for vehicle detection\n",
    "y_stop = 680\n",
    "# scaling factor for sliding detection window\n",
    "scale = 1.1\n",
    "\n",
    "#draw search area frame\n",
    "search_area_image = cv2.rectangle(np.copy(image),(5, y_start),(image.shape[1],y_stop),(0,255,0),5) \n",
    " \n",
    "# look for initial detections using trained classifier \n",
    "matched_windows = find_car_matched_windows(image, COLOR_SPACE, y_start, y_stop, scale, svc, X_scaler,\n",
    "                               N_ORIENTATIONS, N_PIXELS_PER_CELL, N_CELLS_PER_BLOCK, SPATIAL_SIZE, N_HISTOGRAM_BINS)\n",
    "# draw matched search windows on test image\n",
    "matched_windows_image = overlay_rectangles_on_image(image, matched_windows, (0, 0, 255), 6)\n",
    "display_images_in_table([search_area_image, matched_windows_image],[\"Search area\", \"Matched windows\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#9. Using for multiple search space configurations for improving detection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Improve coverage of bounding boxes by combining best (empirically selected) results of\n",
    "# sliding window search for various permutations of scale and search areas\n",
    "search_space_configs = [(463,700,0.97), (502, 680, 1.1), (504, 583, 1.06), (504, 583, 1.09), (512,548,0.54)]\n",
    "\n",
    "matched_windows = []\n",
    "image = mpimg.imread('./test_images/bbox-example-image.jpg')\n",
    "search_areas_image = np.copy(image)\n",
    "matched_windows_image = np.copy(image)\n",
    "for search_config in search_space_configs:\n",
    "    line_color = (np.random.randint(50,255),np.random.randint(50,255), np.random.randint(50,255))\n",
    "    # daw search area for current search space config\n",
    "    search_area = (5, search_config[0]),(image.shape[1], search_config[1])\n",
    "    search_areas_image = cv2.rectangle(search_areas_image, search_area[0], search_area[1], line_color, 4)\n",
    "    # look for car matched windows using trained classifier \n",
    "    matched_windows_frame = find_car_matched_windows(image, COLOR_SPACE, search_config[0], search_config[1],\n",
    "                                       search_config[2], svc, X_scaler, N_ORIENTATIONS, N_PIXELS_PER_CELL, N_CELLS_PER_BLOCK, SPATIAL_SIZE, N_HISTOGRAM_BINS)\n",
    "    matched_windows_image = overlay_rectangles_on_image(matched_windows_image, matched_windows_frame, line_color, 4)\n",
    "    matched_windows.extend(matched_windows_frame)\n",
    "    \n",
    "display_images_in_table([search_areas_image, matched_windows_image],[\"Search areas\", \"Matched windows\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#10. Group matched detection windows using heatmap with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "def add_heat_to_heatmap(heatmap, bounding_boxes):\n",
    "    for bounding_box in bounding_boxes:\n",
    "        # Add +1 for all pixels inside each bounding box\n",
    "        # Assuming each \"bounding box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[bounding_box[0][1]:bounding_box[1][1], bounding_box[0][0]:bounding_box[1][0]] += 1\n",
    "    # Return updated heatmap\n",
    "    return heatmap\n",
    "    \n",
    "def apply_threshold_to_heatmap(heatmap, threshold):\n",
    "    # Zero out all pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return updated heatmap\n",
    "    return heatmap\n",
    "\n",
    "def labels_to_bounding_boxes(detected_labels, box_min_width, box_min_height):\n",
    "    bounding_boxes = []\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, detected_labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero_pixels = (detected_labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzero_pixels_x = np.array(nonzero_pixels[1])\n",
    "        nonzero_pixels_y = np.array(nonzero_pixels[0])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        box_left_top = (np.min(nonzero_pixels_x), np.min(nonzero_pixels_y))\n",
    "        box_right_bottom = (np.max(nonzero_pixels_x), np.max(nonzero_pixels_y))\n",
    "        if(((box_right_bottom[0]-box_left_top[0]) >=box_min_width) and ((box_right_bottom[1]-box_left_top[1]) >=box_min_height)):\n",
    "            bounding_boxes.append((box_left_top, box_right_bottom))\n",
    "    return bounding_boxes\n",
    "\n",
    "def get_combined_bounding_boxes(image, windows, threshold=0, box_min_width=0, box_min_height=0):\n",
    "    \n",
    "    heatmap = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "    # Add heat to each window in windows list\n",
    "    heatmap = add_heat_to_heatmap(heatmap, windows)  \n",
    "    # Apply threshold to help remove false positives\n",
    "    heatmap = apply_threshold_to_heatmap(heatmap, threshold)\n",
    "\n",
    "    # Find final boxes from heatmap using label function\n",
    "    heat_labels = label(heatmap)\n",
    "    return labels_to_bounding_boxes(heat_labels, box_min_width, box_min_height), heatmap\n",
    "\n",
    "# threshold for adjusting heatmap to skip false positive windows\n",
    "HEAT_THRESHOLD = 2\n",
    "\n",
    "# additional thresholds for the size of detection bounding boxes\n",
    "BOX_MIN_WIDTH=55\n",
    "BOX_MIN_HEIGHT=55\n",
    "\n",
    "image = mpimg.imread('./test_images/bbox-example-image.jpg')\n",
    "\n",
    "# generate bounding boxes that cover all matched sliding windows\n",
    "bounding_boxes, heatmap = get_combined_bounding_boxes(image, matched_windows, heat_threshold)\n",
    "# draw detected car bounding boxes on test image\n",
    "bounding_boxes_image = overlay_rectangles_on_image(image, bounding_boxes, (0, 255, 0), 6)\n",
    "display_images_in_table([matched_windows_image, bounding_boxes_image, heatmap],[\"Matched sliding windows\", \"Combined bounding boxes\", \"Heatmap\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#11. Manual tunning of search space and filtering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import IntRangeSlider\n",
    "from ipywidgets import IntSlider\n",
    "from ipywidgets import FloatSlider\n",
    "from ipywidgets import RadioButtons\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "test_image_filenames = glob.glob('./test_images/*.jpg') \n",
    "image_selection_radio=RadioButtons(options=test_image_filenames, value=test_image_filenames[0], continuous_update=False, description=\"Image_file\")\n",
    "search_area_change_slider = IntRangeSlider(min=350, max=700, step=1, value=[390,600], continuous_update=False, description=\"Search area\")\n",
    "scale_change_slider = FloatSlider(min=0.3, max=3.5, step=0.01, value=1.5, continuous_update=False, description=\"Scale\")\n",
    "heat_threshold_change_slider = IntSlider(min=0, max=4, step=1, value=0, continuous_update=False, description=\"Heatmap threshold\")\n",
    "box_min_width_change_slider = IntSlider(min=0, max=200, step=10, value=80, continuous_update=False, description=\"Box min with\")\n",
    "box_min_height_change_slider = IntSlider(min=0, max=200, step=10, value=50, continuous_update=False, description=\"Box min height\")\n",
    "\n",
    "\n",
    "def display_search_car_matching_windows(image_file, y_threshold=(400,700), scale=1.0, heat_threshold=0, box_min_width=0, box_min_height=0):\n",
    "    image = mpimg.imread(image_file)\n",
    "    \n",
    "    # draw search area\n",
    "    search_area = (5, y_threshold[0]),(image.shape[1], y_threshold[1])\n",
    "    search_area_image = cv2.rectangle(np.copy(image), search_area[0], search_area[1], (255,0,0), 3)\n",
    "    \n",
    "    # look for car matched windows using trained classifier \n",
    "    matched_windows_frame = find_car_matched_windows(image, COLOR_SPACE, y_threshold[0], y_threshold[1], scale,\n",
    "                                                     svc, X_scaler, N_ORIENTATIONS, N_PIXELS_PER_CELL,\n",
    "                                                     N_CELLS_PER_BLOCK, SPATIAL_SIZE, N_HISTOGRAM_BINS)\n",
    "    matched_windows_image = overlay_rectangles_on_image(search_area_image, matched_windows_frame, (0,255,0), 3)\n",
    "    \n",
    "    # generate bounding boxes that cover all matched sliding windows\n",
    "    bounding_boxes, heatmap = get_combined_bounding_boxes(image, matched_windows_frame, heat_threshold, box_min_width, box_min_height)\n",
    "    # draw detected car bounding boxes on test image\n",
    "    bounding_boxes_image = overlay_rectangles_on_image(image, bounding_boxes, (0,0,255), 6)\n",
    "    \n",
    "    display_images_in_table([matched_windows_image, bounding_boxes_image],\n",
    "                            [\"Search area Y lmits: {}-{}. Scale: {}, \".format(y_threshold[0], y_threshold[1], scale),\n",
    "                             \"Bounding boxes. Heatmap threshold: {}, size threshold: {}x{}\".format(heat_threshold, box_min_width, box_min_height)])\n",
    "\n",
    "interact(display_search_car_matching_windows,\n",
    "         image_file = image_selection_radio,\n",
    "         y_threshold = search_area_change_slider,\n",
    "         scale = scale_change_slider,\n",
    "         heat_threshold=heat_threshold_change_slider,\n",
    "         box_min_width=box_min_width_change_slider,\n",
    "         box_min_height=box_min_height_change_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#12. Detecting of the bounding boxes on 'challenging' cases from test video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# search space configurations for detecting vehilces in target video file\n",
    "SEARCH_SPACE_CONFIGS = [(395,488,1.32),(431,518,1.35),(405, 499, 1.20), (405, 499, 1.06), (401, 499, 0.93), (392,522,2.01), (393,510,1.83), (415,499,1.0), (389,511,1.34), (401,489,1.33), (392,608,0.9)]\n",
    "\n",
    "def detect_vehicles(image, vis_not_filtered=False):\n",
    "    matched_windows = []\n",
    "    for search_config in SEARCH_SPACE_CONFIGS:\n",
    "        # look for car matched windows using trained classifier \n",
    "        windows = find_car_matched_windows(image, COLOR_SPACE, search_config[0], search_config[1], search_config[2],\n",
    "                                           svc, X_scaler, N_ORIENTATIONS, N_PIXELS_PER_CELL, N_CELLS_PER_BLOCK, SPATIAL_SIZE, N_HISTOGRAM_BINS)\n",
    "        if(len(windows) > 0):\n",
    "            matched_windows.extend(windows)\n",
    "     \n",
    "    bounding_boxes, heatmap =  get_combined_bounding_boxes(image, matched_windows, HEAT_THRESHOLD,BOX_MIN_WIDTH, BOX_MIN_HEIGHT)\n",
    "    b_boxes_image = overlay_rectangles_on_image(image, bounding_boxes, (0,255,0), 6)\n",
    "    if(vis_not_filtered == True):\n",
    "        not_filtered_bounding_boxes, heatmap =  get_combined_bounding_boxes(image, matched_windows, 0,0,0)\n",
    "        not_filtered_b_boxes_image = overlay_rectangles_on_image(image, not_filtered_bounding_boxes, (0,255,0), 6)\n",
    "        return b_boxes_image, not_filtered_b_boxes_image\n",
    "    else:\n",
    "        return b_boxes_image\n",
    "\n",
    "\n",
    "test_image_filenames = glob.glob('./test_images/test*.jpg') \n",
    "processed_images = []\n",
    "image_labels = []\n",
    "for test_image_file in test_image_filenames:\n",
    "    test_image = mpimg.imread(test_image_file)\n",
    "    filtered_image, not_filtered_image = detect_vehicles(test_image, vis_not_filtered=True)\n",
    "    display_images_in_table([not_filtered_image, filtered_image],['not filtered detection', 'filtered detection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "# PROCESS VIDEO\n",
    "video_in_file = './test_video.mp4'\n",
    "video_out_file = './test_video_out.mp4'\n",
    "\n",
    "video_in = VideoFileClip(video_in_file)\n",
    "video_out = video_in.fl_image(detect_vehicles)\n",
    "%time video_out.write_videofile(video_out_file, audio=False)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(video_out_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#13. Detecting a cars in target video file with multiframe detection smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MULTIFRAME VEHICLE DETECTION\n",
    "class MultiframeDetectionCache():\n",
    "    def __init__(self, cache_size):\n",
    "        self.contents = []\n",
    "        self.max_size = cache_size\n",
    "        \n",
    "    def update(self, frame_detections):\n",
    "        self.contents.append(frame_detections)\n",
    "        # constantly refresh the content of the cache by cutting out 'too old' detections\n",
    "        self.contents = self.contents[-self.max_size:]\n",
    "\n",
    "def get_combined_traced_bounding_boxes(image, windows_cache, frame_threshold, box_min_width=0, box_min_height=0):\n",
    "    \n",
    "    heatmap = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "    # Add heat to each window in windows list\n",
    "    for frame_windows in windows_cache.contents:\n",
    "        # update resulting heatmap with heat from cached frame\n",
    "        heatmap = add_heat_to_heatmap(heatmap, frame_windows)  \n",
    "    \n",
    "    # Apply the threshold to remove false positives\n",
    "    heatmap = apply_threshold_to_heatmap(heatmap, frame_threshold * windows_cache.max_size)\n",
    "\n",
    "    # Find final boxes from heatmap using label function\n",
    "    heat_labels = label(heatmap)\n",
    "    return labels_to_bounding_boxes(heat_labels, box_min_width, box_min_height), heatmap        \n",
    "        \n",
    "def detect_vehicles_traced(image):\n",
    "    matched_windows = []\n",
    "    for search_config in SEARCH_SPACE_CONFIGS:\n",
    "        # look for car matched windows using trained classifier \n",
    "        windows = find_car_matched_windows(image, COLOR_SPACE, search_config[0], search_config[1],\n",
    "                                           search_config[2], svc, X_scaler, N_ORIENTATIONS, N_PIXELS_PER_CELL,\n",
    "                                           N_CELLS_PER_BLOCK, SPATIAL_SIZE, N_HISTOGRAM_BINS)\n",
    "        if(len(windows) > 0):\n",
    "            matched_windows.extend(windows)\n",
    "            \n",
    "    detection_cache.update(matched_windows)        \n",
    "    \n",
    "    bounding_boxes, heatmap =  get_combined_traced_bounding_boxes(image, detection_cache, HEAT_THRESHOLD, BOX_MIN_WIDTH, BOX_MIN_HEIGHT)\n",
    "    b_boxes_image = overlay_rectangles_on_image(image, bounding_boxes, (0,255,0), 6)\n",
    "    return b_boxes_image \n",
    "\n",
    "\n",
    "detection_cache = MultiframeDetectionCache(cache_size=15)\n",
    "video_in_file = './project_video.mp4'\n",
    "video_out_file = './project_video_out.mp4'\n",
    "\n",
    "video_in = VideoFileClip(video_in_file)#.subclip(27,32) \n",
    "video_out = video_in.fl_image(detect_vehicles_traced)\n",
    "%time video_out.write_videofile(video_out_file, audio=False)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(video_out_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
